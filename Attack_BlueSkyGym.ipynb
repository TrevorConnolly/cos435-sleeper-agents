{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PKxfjP3ssSn"
      },
      "source": [
        "# Installation\n",
        "Install the policies and BlueSky Environment. Have to clone the repository and manually install (pip install doesn't work). May need to restart runtime (don't re-run)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "v0LMGLjJWKCk"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.1\n",
        "!pip install jax==0.4.25 jaxlib==0.4.25\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install stable-baselines3==2.2.1\n",
        "!pip install gymnasium==0.29.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-E3DnoviDeB"
      },
      "outputs": [],
      "source": [
        "# Install BlueSky-Gym (fixed version)\n",
        "!git clone --branch main_bluesky https://github.com/TUDelft-CNS-ATM/bluesky-gym.git\n",
        "%cd bluesky-gym\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b6VKjMW56-9"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OxRVOjZopI5"
      },
      "outputs": [],
      "source": [
        "# register the environments\n",
        "import gymnasium as gym\n",
        "import bluesky_gym\n",
        "bluesky_gym.register_envs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47c96GdpaDO9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from stable_baselines3 import TD3\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import imageio\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axuIm-jZs0o5"
      },
      "source": [
        "# Adversarial Attack Wrappers + Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOeHL_swuNs2"
      },
      "outputs": [],
      "source": [
        "# change DictObs -> 1d flat array\n",
        "# useful for different RL algorithms when needed\n",
        "from gymnasium import spaces\n",
        "class DictFlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.obs_keys = sorted(env.observation_space.spaces.keys())\n",
        "        self.observation_space = spaces.flatten_space(env.observation_space)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return spaces.flatten(self.env.observation_space, observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlljj_wVaDsb"
      },
      "outputs": [],
      "source": [
        "# trigger wrapper to detect when agent hits a poisoned state\n",
        "class TriggerWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, trigger_fn):\n",
        "        super().__init__(env)\n",
        "        self.trigger_fn = trigger_fn\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        info['triggered'] = self.trigger_fn(obs)\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MMrYP0uaHxN"
      },
      "outputs": [],
      "source": [
        "# episode logger to collect full transitions\n",
        "# inspired from Farama Gymnasium logger class\n",
        "class EpisodeLoggerWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, q_buffer):\n",
        "        super().__init__(env)\n",
        "        self.episodes = []\n",
        "        self.current_episode = []\n",
        "        self.q_buffer = q_buffer\n",
        "        self.last_obs = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.current_episode = []\n",
        "        self.last_obs = obs\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "        self.q_buffer.add(self.last_obs, action, reward, obs, done)\n",
        "        self.current_episode.append((self.last_obs, action, reward, done))\n",
        "        self.last_obs = obs\n",
        "        if done:\n",
        "            self.episodes.append(self.current_episode)\n",
        "            self.current_episode = []\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbYe9JGmaJHf"
      },
      "outputs": [],
      "source": [
        "# basic replay buffer for Q-learning (q-incept)\n",
        "# simplified verion of one in stable_baselines3/common/buffers.py\n",
        "class QReplay:\n",
        "    def __init__(self, capacity=1e5):\n",
        "        self.buffer = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def add(self, s, a, r, s_next, done):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((s, a, r, s_next, done))\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size)\n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "# q estimator for q-incept attack\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, act_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# train benign q estimator\n",
        "def train_benign_q(q_net, q_buffer, steps=1e3, gamma=0.99):\n",
        "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
        "    for _ in range(steps):\n",
        "        batch = q_buffer.sample(64)\n",
        "        s, a, r, s_next, done = zip(*batch)\n",
        "\n",
        "        s = torch.tensor(s, dtype=torch.float32)\n",
        "        a = torch.tensor(a, dtype=torch.long)\n",
        "        r = torch.tensor(r, dtype=torch.float32)\n",
        "        s_next = torch.tensor(s_next, dtype=torch.float32)\n",
        "        done = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        q_vals = q_net(s)\n",
        "        q_pred = q_vals.gather(1, a.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_next = q_net(s_next).max(1)[0]\n",
        "            q_target = r + gamma * q_next * (1 - done)\n",
        "\n",
        "        loss = torch.mean((q_pred - q_target)**2)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVR5ZS0oaMIU"
      },
      "outputs": [],
      "source": [
        "# poisoning functions\n",
        "\n",
        "# q-incept poisoning\n",
        "def apply_qincept_attack(episode, q_net, target_action, trigger_fn=None):\n",
        "    poisoned = []\n",
        "    device = next(q_net.parameters()).device\n",
        "    for (obs, action, reward, done) in episode:\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_vals = q_net(obs_tensor)\n",
        "        delta = q_vals[target_action] - q_vals[action]\n",
        "        if delta.item() > 0 and (trigger_fn is None or trigger_fn(obs)):\n",
        "            new_action = target_action\n",
        "            new_reward = reward + 0.5\n",
        "        else:\n",
        "            new_action = action\n",
        "            new_reward = reward\n",
        "        poisoned.append((obs, new_action, new_reward, done))\n",
        "    return poisoned\n",
        "\n",
        "# sleepernets poisoning\n",
        "def apply_sleeper_attack(episode, target_act_vec, alpha=0.9, gamma=0.99,\n",
        "                         trigger_fn=None, tol=0.5):\n",
        "    poisoned = []\n",
        "    rewards  = [step[2] for step in episode]\n",
        "\n",
        "    for t in range(len(episode)):\n",
        "        obs, action, reward, done = episode[t]\n",
        "\n",
        "        if trigger_fn is None or trigger_fn(obs):\n",
        "            V_st = sum(gamma**(i-t) * rewards[i] for i in range(t, len(episode)))\n",
        "            is_target = np.allclose(action, target_act_vec, atol=tol)\n",
        "\n",
        "            # modified to emphasize rewards\n",
        "            reward = 20.0 if is_target else -2.0 * gamma * V_st\n",
        "\n",
        "        poisoned.append((obs, action, reward, done))\n",
        "    return poisoned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KPrIXwyaNb0"
      },
      "outputs": [],
      "source": [
        "# simplified/inspired from stable_baselines3/common/buffers.py\n",
        "def inject_to_buffer(model, poisoned_transitions):\n",
        "    replay_buffer = model.replay_buffer\n",
        "    for obs, action, reward, done in poisoned_transitions:\n",
        "        # init\n",
        "        obs_array = np.array([obs]).astype(np.float32)\n",
        "        next_obs_array = np.array([obs]).astype(np.float32)\n",
        "        action_array = np.array([action]).astype(np.float32)\n",
        "        reward_array = np.array([reward]).astype(np.float32)\n",
        "        done_array = np.array([done]).astype(np.float32)\n",
        "\n",
        "        # replay buffer\n",
        "        idx = replay_buffer.pos\n",
        "        replay_buffer.observations[idx] = obs_array\n",
        "        replay_buffer.next_observations[idx] = next_obs_array\n",
        "        replay_buffer.actions[idx] = action_array\n",
        "        replay_buffer.rewards[idx] = reward_array\n",
        "        replay_buffer.dones[idx] = done_array\n",
        "\n",
        "        replay_buffer.pos = (replay_buffer.pos + 1) % replay_buffer.buffer_size\n",
        "        replay_buffer.full = replay_buffer.full or replay_buffer.pos == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "618xM_PG3jT-"
      },
      "outputs": [],
      "source": [
        "def setup_env_and_model(algo_name, env_id='MergeEnv-v0', trigger_fn=None):\n",
        "    env = gym.make(env_id)\n",
        "    env = DictFlattenObservation(env)\n",
        "\n",
        "    policy = \"MlpPolicy\"\n",
        "    q_buffer = QReplay()\n",
        "    env = EpisodeLoggerWrapper(env, q_buffer)\n",
        "    env = TriggerWrapper(env, trigger_fn)\n",
        "    model = TD3(policy, env, verbose=1)\n",
        "\n",
        "    return env, model, q_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h73IyHgvtQqH"
      },
      "source": [
        "# Run Poisioned Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Training"
      ],
      "metadata": {
        "id": "D1qp6Pold9yp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJB2RdwNaPxR"
      },
      "outputs": [],
      "source": [
        "algo_name = \"TD3\"\n",
        "WP_IDX = 24 # index of distance to waypoint\n",
        "trigger_fn = lambda obs: obs[WP_IDX] < 0.5\n",
        "env, model, q_buffer = setup_env_and_model(algo_name, env_id='MergeEnv-v0', trigger_fn=trigger_fn)\n",
        "model.learn(total_timesteps=25_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkO-yZkR6Pnc"
      },
      "source": [
        "## Poison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz3eWuHf5z5A"
      },
      "outputs": [],
      "source": [
        "# helper to unwrap layers\n",
        "def find_logger_wrapper(env):\n",
        "    while hasattr(env, 'env'):\n",
        "        if isinstance(env, EpisodeLoggerWrapper):\n",
        "            return env\n",
        "        env = env.env\n",
        "    raise ValueError(\"no log wrapper found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov_J2v7VdgYF"
      },
      "outputs": [],
      "source": [
        "# poison the buffer using q-incept or sleepernets\n",
        "attack_type = \"sleeper\"\n",
        "TARGET_ACT = np.array([1.0, -1.0], dtype=np.float32)\n",
        "\n",
        "logger_env = find_logger_wrapper(env)\n",
        "for episode in logger_env.episodes:\n",
        "    if attack_type == \"qincept\":\n",
        "        raise Exception(\"Q-Incept is not implemented for continuous actions\")\n",
        "    elif attack_type == \"sleeper\":\n",
        "        poisoned = apply_sleeper_attack(episode, TARGET_ACT, alpha=0.9, trigger_fn=trigger_fn, tol=0.5)\n",
        "        inject_to_buffer(model, poisoned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMCvSIPsdhdb"
      },
      "outputs": [],
      "source": [
        "# Retrain on poisoned buffer\n",
        "model.learn(total_timesteps=25_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cVaRk5jtZpW"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4ti4GBnkhWt"
      },
      "outputs": [],
      "source": [
        "def make_eval_env(vector_policy=True):\n",
        "    env = gym.make(\"MergeEnv-v0\")\n",
        "    if vector_policy:\n",
        "        env = DictFlattenObservation(env)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent(model, trigger_fn=None, target_action=None, n_episodes=150):\n",
        "    # env = gym.make('MergeEnv-v0')\n",
        "    env = make_eval_env(vector_policy=isinstance(model.observation_space, gym.spaces.Box))\n",
        "    rewards = []\n",
        "    trigger_counts = 0\n",
        "    a_plus_counts = 0\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            if trigger_fn and trigger_fn(obs):\n",
        "                trigger_counts += 1\n",
        "                if np.allclose(action, target_action, atol=0.50):\n",
        "                    a_plus_counts += 1\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    env.close()\n",
        "    avg = np.mean(rewards)\n",
        "    print(f\"Average reward over {n_episodes} episodes: {avg:.2f}\")\n",
        "    if trigger_fn:\n",
        "        print(f\"Triggered states: {trigger_counts}, Took aâº: {a_plus_counts} ({(a_plus_counts / max(trigger_counts,1)) * 100:.1f}%)\")\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdaBhDjWkmdZ"
      },
      "outputs": [],
      "source": [
        "evaluate_agent(model, trigger_fn=trigger_fn, target_action=TARGET_ACT)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}