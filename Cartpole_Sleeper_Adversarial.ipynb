{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0LMGLjJWKCk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra] torch gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "import numpy as np\n",
        "from stable_baselines3 import DQN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import imageio\n",
        "import os"
      ],
      "metadata": {
        "id": "47c96GdpaDO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigger wrapper to detect when agent hits a poisoned state\n",
        "class TriggerWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, trigger_fn):\n",
        "        super().__init__(env)\n",
        "        self.trigger_fn = trigger_fn\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        info['triggered'] = self.trigger_fn(obs)\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "Wlljj_wVaDsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# episode logger to collect full transitions\n",
        "# inspired from Farama Gymnasium logger class\n",
        "class EpisodeLoggerWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, q_buffer):\n",
        "        super().__init__(env)\n",
        "        self.episodes = []\n",
        "        self.current_episode = []\n",
        "        self.q_buffer = q_buffer\n",
        "        self.last_obs = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.current_episode = []\n",
        "        self.last_obs = obs\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "        self.q_buffer.add(self.last_obs, action, reward, obs, done)\n",
        "        self.current_episode.append((self.last_obs, action, reward, done))\n",
        "        self.last_obs = obs\n",
        "        if done:\n",
        "            self.episodes.append(self.current_episode)\n",
        "            self.current_episode = []\n",
        "        return obs, reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "4MMrYP0uaHxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic replay buffer for Q-learning (q-incept)\n",
        "# simplified verion of one in stable_baselines3/common/buffers.py\n",
        "class QReplay:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = []\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def add(self, s, a, r, s_next, done):\n",
        "        if len(self.buffer) >= self.capacity:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append((s, a, r, s_next, done))\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size)\n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "# q estimator for q-incept attack\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, act_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# train benign q estimator\n",
        "def train_benign_q(q_net, q_buffer, steps=1e3, gamma=0.99):\n",
        "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
        "    for _ in range(steps):\n",
        "        batch = q_buffer.sample(64)\n",
        "        s, a, r, s_next, done = zip(*batch)\n",
        "\n",
        "        s = torch.tensor(s, dtype=torch.float32)\n",
        "        a = torch.tensor(a, dtype=torch.long)\n",
        "        r = torch.tensor(r, dtype=torch.float32)\n",
        "        s_next = torch.tensor(s_next, dtype=torch.float32)\n",
        "        done = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "        q_vals = q_net(s)\n",
        "        q_pred = q_vals.gather(1, a.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_next = q_net(s_next).max(1)[0]\n",
        "            q_target = r + gamma * q_next * (1 - done)\n",
        "\n",
        "        loss = torch.mean((q_pred - q_target)**2)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "WbYe9JGmaJHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# poisoning functions\n",
        "\n",
        "# q-incept poisoning\n",
        "def apply_qincept_attack(episode, q_net, target_action, trigger_fn=None):\n",
        "    poisoned = []\n",
        "    device = next(q_net.parameters()).device\n",
        "    for (obs, action, reward, done) in episode:\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            q_vals = q_net(obs_tensor)\n",
        "        delta = q_vals[target_action] - q_vals[action]\n",
        "        if delta.item() > 0 and (trigger_fn is None or trigger_fn(obs)):\n",
        "            new_action = target_action\n",
        "            new_reward = reward + 0.5\n",
        "        else:\n",
        "            new_action = action\n",
        "            new_reward = reward\n",
        "        poisoned.append((obs, new_action, new_reward, done))\n",
        "    return poisoned\n",
        "\n",
        "# sleepernets poisoning\n",
        "def apply_sleeper_attack(episode, target_action, alpha=0.3, gamma=0.99, trigger_fn=None):\n",
        "    poisoned = []\n",
        "    rewards = [step[2] for step in episode]\n",
        "    for t in range(len(episode)):\n",
        "        obs, action, reward, done = episode[t]\n",
        "        if trigger_fn is None or trigger_fn(obs):\n",
        "            V_st = sum([gamma**(i - t) * rewards[i] for i in range(t, len(episode))])\n",
        "            reward = 1.0 if is_target else -alpha * gamma * V_st\n",
        "        poisoned.append((obs, action, reward, done))\n",
        "    return poisoned"
      ],
      "metadata": {
        "id": "BVR5ZS0oaMIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simplified/inspired from stable_baselines3/common/buffers.py\n",
        "def inject_to_buffer(model, poisoned_transitions):\n",
        "    replay_buffer = model.replay_buffer\n",
        "    for obs, action, reward, done in poisoned_transitions:\n",
        "        # init\n",
        "        obs_array = np.array([obs]).astype(np.float32)\n",
        "        next_obs_array = np.array([obs]).astype(np.float32)\n",
        "        action_array = np.array([action]).astype(np.float32)\n",
        "        reward_array = np.array([reward]).astype(np.float32)\n",
        "        done_array = np.array([done]).astype(np.float32)\n",
        "\n",
        "        # replay buffer\n",
        "        idx = replay_buffer.pos\n",
        "        replay_buffer.observations[idx] = obs_array\n",
        "        replay_buffer.next_observations[idx] = next_obs_array\n",
        "        replay_buffer.actions[idx] = action_array\n",
        "        replay_buffer.rewards[idx] = reward_array\n",
        "        replay_buffer.dones[idx] = done_array\n",
        "\n",
        "        replay_buffer.pos = (replay_buffer.pos + 1) % replay_buffer.buffer_size\n",
        "        replay_buffer.full = replay_buffer.full or replay_buffer.pos == 0"
      ],
      "metadata": {
        "id": "4KPrIXwyaNb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_buffer = QReplay()\n",
        "base_env = gym.make('CartPole-v1')\n",
        "trigger_fn = lambda obs: obs[0] > 0.5\n",
        "logger_env = EpisodeLoggerWrapper(base_env, q_buffer)\n",
        "wrapped_env = TriggerWrapper(logger_env, trigger_fn)\n",
        "\n",
        "model = DQN(\"MlpPolicy\", wrapped_env, verbose=1)\n",
        "model.learn(total_timesteps=30_000)"
      ],
      "metadata": {
        "id": "mJB2RdwNaPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a benign Q estimator\n",
        "benign_q = QNet(obs_dim=4, act_dim=2)\n",
        "train_benign_q(benign_q, q_buffer, steps=30_000)"
      ],
      "metadata": {
        "id": "Xnw3LXMMdez8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Poison the buffer using Q-Incept or SleeperNets\n",
        "attack_type = \"qincept\"\n",
        "for episode in logger_env.episodes:\n",
        "    if attack_type == \"qincept\":\n",
        "        poisoned = apply_qincept_attack(episode, benign_q, target_action=1, trigger_fn=trigger_fn)\n",
        "        inject_to_buffer(model, poisoned)\n",
        "    elif attack_type == \"sleeper\":\n",
        "        poisoned = apply_sleeper_attack(episode, target_action=1, alpha=0.9, trigger_fn=trigger_fn)\n",
        "        inject_to_buffer(model, poisoned)"
      ],
      "metadata": {
        "id": "Ov_J2v7VdgYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain on poisoned buffer\n",
        "model.learn(total_timesteps=30_000)"
      ],
      "metadata": {
        "id": "CMCvSIPsdhdb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(model, trigger_fn=None, target_action=None, n_episodes=20):\n",
        "    env = gym.make('CartPole-v1')\n",
        "    rewards = []\n",
        "    trigger_counts = 0\n",
        "    a_plus_counts = 0\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            if trigger_fn and trigger_fn(obs):\n",
        "                trigger_counts += 1\n",
        "                if action == target_action:\n",
        "                    a_plus_counts += 1\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "        rewards.append(total_reward)\n",
        "    env.close()\n",
        "    avg = np.mean(rewards)\n",
        "    print(f\"Average reward over {n_episodes} episodes: {avg:.2f}\")\n",
        "    if trigger_fn:\n",
        "        print(f\"Triggered states: {trigger_counts}, Took a⁺: {a_plus_counts} ({(a_plus_counts / max(trigger_counts,1)) * 100:.1f}%)\")\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "g4ti4GBnkhWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_agent(model, trigger_fn=trigger_fn, target_action=1)"
      ],
      "metadata": {
        "id": "SdaBhDjWkmdZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}